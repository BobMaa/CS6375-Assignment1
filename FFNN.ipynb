{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ccb5c1-6f1f-4137-84fb-24deb9b9efa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from argparse import ArgumentParser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9391cd6d-60cc-4760-ba18-1d6ace17467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = '<UNK>'\n",
    "# Consult the PyTorch documentation for information on the functions used below:\n",
    "# https://pytorch.org/docs/stable/torch.html\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, h):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.h = h\n",
    "        self.W1 = nn.Linear(input_dim, h)\n",
    "        self.activation = nn.ReLU() # The rectified linear unit; one valid choice of activation function\n",
    "        self.output_dim = 5\n",
    "        self.W2 = nn.Linear(h, self.output_dim)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax() # The softmax function that converts vectors into probability distributions; computes log probabilities for computational benefits\n",
    "        self.loss = nn.NLLLoss() # The cross-entropy/negative log likelihood loss taught in class\n",
    "\n",
    "    def compute_Loss(self, predicted_vector, gold_label):\n",
    "        return self.loss(predicted_vector, gold_label)\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        # [to fill] obtain first hidden layer representation\n",
    "        hidden_layer = self.activation(self.W1(input_vector))\n",
    "        # [to fill] obtain output layer representation\n",
    "        output_layer = self.W2(hidden_layer)\n",
    "        # [to fill] obtain probability dist.\n",
    "        predicted_vector = self.softmax(output_layer)\n",
    "        return predicted_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538b1bb3-b939-4cda-a7f0-824c500be788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns: \n",
    "# vocab = A set of strings corresponding to the vocabulary\n",
    "def make_vocab(data):\n",
    "    vocab = set()\n",
    "    for document, _ in data:\n",
    "        for word in document:\n",
    "            vocab.add(word)\n",
    "    return vocab \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vocab = A set of strings corresponding to the vocabulary including <UNK>\n",
    "# word2index = A dictionary mapping word/token to its index (a number in 0, ..., V - 1)\n",
    "# index2word = A dictionary inverting the mapping of word2index\n",
    "def make_indices(vocab):\n",
    "    vocab_list = sorted(vocab)\n",
    "    vocab_list.append(unk)\n",
    "    word2index = {}\n",
    "    index2word = {}\n",
    "    for index, word in enumerate(vocab_list):\n",
    "        word2index[word] = index \n",
    "        index2word[index] = word \n",
    "    vocab.add(unk)\n",
    "    return vocab, word2index, index2word \n",
    "\n",
    "\n",
    "# Returns:\n",
    "# vectorized_data = A list of pairs (vector representation of input, y)\n",
    "def convert_to_vector_representation(data, word2index):\n",
    "    vectorized_data = []\n",
    "    for document, y in data:\n",
    "        vector = torch.zeros(len(word2index)) \n",
    "        for word in document:\n",
    "            index = word2index.get(word, word2index[unk])\n",
    "            vector[index] += 1\n",
    "        vectorized_data.append((vector, y))\n",
    "    return vectorized_data\n",
    "\n",
    "\n",
    "\n",
    "def load_data(train_data, val_data):\n",
    "    with open(train_data) as training_f:\n",
    "        training = json.load(training_f)\n",
    "    with open(val_data) as valid_f:\n",
    "        validation = json.load(valid_f)\n",
    "\n",
    "    tra = []\n",
    "    val = []\n",
    "    for elt in training:\n",
    "        tra.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "    for elt in validation:\n",
    "        val.append((elt[\"text\"].split(),int(elt[\"stars\"]-1)))\n",
    "\n",
    "    return tra, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99e04d9-d92a-4d37-9622-ed47d85deb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Loading data ==========\n",
      "========== Vectorizing data ==========\n"
     ]
    }
   ],
   "source": [
    " # fix random seeds\n",
    "#parser = ArgumentParser()\n",
    "#args = parser.parse_args()\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "train_data = './Data_Embedding/training.json'\n",
    "valid_data = './Data_Embedding/validation.json'\n",
    "# load data\n",
    "print(\"========== Loading data ==========\")\n",
    "train_data, valid_data = load_data(train_data, valid_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "vocab = make_vocab(train_data)\n",
    "vocab, word2index, index2word = make_indices(vocab)\n",
    "\n",
    "print(\"========== Vectorizing data ==========\")\n",
    "train_data = convert_to_vector_representation(train_data, word2index)\n",
    "valid_data = convert_to_vector_representation(valid_data, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9626d2a1-5a6a-44e1-9669-4374cec57ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNN(input_dim = len(vocab), h = 100)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6c3c7a-c76e-436e-a0bd-36d127b64b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Training for 5 epochs ==========\n",
      "Training started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]C:\\Users\\40119\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:53<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 1\n",
      "Training accuracy for epoch 1: 0.421\n",
      "Training time for this epoch: 173.23645496368408\n",
      "Validation started for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 26.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 1\n",
      "Validation accuracy for epoch 1: 0.52875\n",
      "Validation time for this epoch: 1.8582541942596436\n",
      "Training started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:32<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 2\n",
      "Training accuracy for epoch 2: 0.5056875\n",
      "Training time for this epoch: 272.6725826263428\n",
      "Validation started for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 2\n",
      "Validation accuracy for epoch 2: 0.505\n",
      "Validation time for this epoch: 1.9642338752746582\n",
      "Training started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:00<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 3\n",
      "Training accuracy for epoch 3: 0.541\n",
      "Training time for this epoch: 300.1188385486603\n",
      "Validation started for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 3\n",
      "Validation accuracy for epoch 3: 0.59125\n",
      "Validation time for this epoch: 1.9412829875946045\n",
      "Training started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:21<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 4\n",
      "Training accuracy for epoch 4: 0.5833125\n",
      "Training time for this epoch: 321.3179190158844\n",
      "Validation started for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 4\n",
      "Validation accuracy for epoch 4: 0.55\n",
      "Validation time for this epoch: 1.923403263092041\n",
      "Training started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:31<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for epoch 5\n",
      "Training accuracy for epoch 5: 0.6125625\n",
      "Training time for this epoch: 331.39816546440125\n",
      "Validation started for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 25.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation completed for epoch 5\n",
      "Validation accuracy for epoch 5: 0.58375\n",
      "Validation time for this epoch: 1.9428677558898926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "print(\"========== Training for {} epochs ==========\".format(epochs))\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    print(\"Training started for epoch {}\".format(epoch + 1))\n",
    "    random.shuffle(train_data) # Good practice to shuffle order of training data\n",
    "    minibatch_size = 16 \n",
    "    N = len(train_data) \n",
    "    for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        for example_index in range(minibatch_size):\n",
    "            input_vector, gold_label = train_data[minibatch_index * minibatch_size + example_index]\n",
    "            predicted_vector = model(input_vector)\n",
    "            predicted_label = torch.argmax(predicted_vector)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "            if loss is None:\n",
    "                loss = example_loss\n",
    "            else:\n",
    "                loss += example_loss\n",
    "        loss = loss / minibatch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Training completed for epoch {}\".format(epoch + 1))\n",
    "    print(\"Training accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "    print(\"Training time for this epoch: {}\".format(time.time() - start_time))\n",
    "    loss = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    print(\"Validation started for epoch {}\".format(epoch + 1))\n",
    "    minibatch_size = 16 \n",
    "    N = len(valid_data) \n",
    "    for minibatch_index in tqdm(range(N // minibatch_size)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = None\n",
    "        for example_index in range(minibatch_size):\n",
    "            input_vector, gold_label = valid_data[minibatch_index * minibatch_size + example_index]\n",
    "            predicted_vector = model(input_vector)\n",
    "            predicted_label = torch.argmax(predicted_vector)\n",
    "            correct += int(predicted_label == gold_label)\n",
    "            total += 1\n",
    "            example_loss = model.compute_Loss(predicted_vector.view(1,-1), torch.tensor([gold_label]))\n",
    "            if loss is None:\n",
    "                loss = example_loss\n",
    "            else:\n",
    "                loss += example_loss\n",
    "        loss = loss / minibatch_size\n",
    "    print(\"Validation completed for epoch {}\".format(epoch + 1))\n",
    "    print(\"Validation accuracy for epoch {}: {}\".format(epoch + 1, correct / total))\n",
    "    print(\"Validation time for this epoch: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914bb08-9d90-4e62-a262-59f6caf20411",
   "metadata": {},
   "outputs": [],
   "source": [
    " # fix random seeds\n",
    "#parser = ArgumentParser()\n",
    "#args = parser.parse_args()\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "test_data = './Data_Embedding/test.json'\n",
    "valid_data = './Data_Embedding/validation.json'\n",
    "# load data\n",
    "print(\"========== Loading data ==========\")\n",
    "test_data, valid_data = load_data(test_data, valid_data) # X_data is a list of pairs (document, y); y in {0,1,2,3,4}\n",
    "vocab = make_vocab(test_data)\n",
    "vocab, word2index, index2word = make_indices(vocab)\n",
    "\n",
    "print(\"========== Vectorizing data ==========\")\n",
    "test_data = convert_to_vector_representation(test_data, word2index)\n",
    "valid_data = convert_to_vector_representation(valid_data, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc3e92-af86-446b-a13e-80c32d05ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50be4c-f6f6-4e3e-b755-50e0de8eb853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[item[0] for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbd08c-8fdf-4c56-aa73-af4c4518f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de8d04-fe75-403c-bd96-5a11f30ec27c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
